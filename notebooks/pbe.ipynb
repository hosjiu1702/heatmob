{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd5f658-67b3-4fae-be2c-aea0fa97f46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetInpaintPipeline, ControlNetModel\n",
    "from controlnet_aux import OpenposeDetector\n",
    "from diffusers.utils import load_image\n",
    "import PIL\n",
    "import torch\n",
    "\n",
    "from src.utilities import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42965ee0-7534-4a48-8b23-11e8ee8ce757",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image_encoder/model.safetensors not found\n",
      "Fetching 8 files: 100%|██████████| 8/8 [01:46<00:00, 13.27s/it]\n",
      "Loading pipeline components...:   0%|          | 0/4 [00:00<?, ?it/s]You are using a model of type clip_vision_model to instantiate a model of type clip. This is not supported for all configurations of models and can yield errors.\n",
      "/workspace/playground/dev-venv/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading pipeline components...:  25%|██▌       | 1/4 [00:01<00:05,  1.68s/it]An error occurred while trying to fetch /home/ubuntu/.cache/huggingface/hub/models--Fantasy-Studio--Paint-by-Example/snapshots/351e6427d8c28a3b24f7c751d43eb4b6735127f7/vae: Error no file named diffusion_pytorch_model.safetensors found in directory /home/ubuntu/.cache/huggingface/hub/models--Fantasy-Studio--Paint-by-Example/snapshots/351e6427d8c28a3b24f7c751d43eb4b6735127f7/vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...:  75%|███████▌  | 3/4 [00:01<00:00,  2.00it/s]An error occurred while trying to fetch /home/ubuntu/.cache/huggingface/hub/models--Fantasy-Studio--Paint-by-Example/snapshots/351e6427d8c28a3b24f7c751d43eb4b6735127f7/unet: Error no file named diffusion_pytorch_model.safetensors found in directory /home/ubuntu/.cache/huggingface/hub/models--Fantasy-Studio--Paint-by-Example/snapshots/351e6427d8c28a3b24f7c751d43eb4b6735127f7/unet.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "Loading pipeline components...: 100%|██████████| 4/4 [00:03<00:00,  1.18it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pipeline <class 'diffusers.pipelines.controlnet.pipeline_controlnet_inpaint.StableDiffusionControlNetInpaintPipeline'> expected {'image_encoder', 'scheduler', 'text_encoder', 'vae', 'safety_checker', 'controlnet', 'unet', 'feature_extractor', 'tokenizer'}, but only {'image_encoder', 'scheduler', 'vae', 'safety_checker', 'unet', 'feature_extractor'} were passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionControlNetInpaintPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFantasy-Studio/Paint-by-Example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_checker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m controlnet \u001b[38;5;241m=\u001b[39m ControlNet\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlllyasviel/sd-controlnet-openpose\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m     11\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m masker \u001b[38;5;241m=\u001b[39m Masker()\n",
      "File \u001b[0;32m/workspace/playground/dev-venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/playground/dev-venv/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:967\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_modules) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    966\u001b[0m     passed_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlist\u001b[39m(init_kwargs\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(passed_class_obj\u001b[38;5;241m.\u001b[39mkeys())) \u001b[38;5;241m-\u001b[39m optional_kwargs\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    969\u001b[0m     )\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# 10. Instantiate the pipeline\u001b[39;00m\n\u001b[1;32m    972\u001b[0m model \u001b[38;5;241m=\u001b[39m pipeline_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Pipeline <class 'diffusers.pipelines.controlnet.pipeline_controlnet_inpaint.StableDiffusionControlNetInpaintPipeline'> expected {'image_encoder', 'scheduler', 'text_encoder', 'vae', 'safety_checker', 'controlnet', 'unet', 'feature_extractor', 'tokenizer'}, but only {'image_encoder', 'scheduler', 'vae', 'safety_checker', 'unet', 'feature_extractor'} were passed."
     ]
    }
   ],
   "source": [
    "pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "    \"Fantasy-Studio/Paint-by-Example\",\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None,\n",
    "    feature_extractor=None,\n",
    ").to('cuda')\n",
    "\n",
    "controlnet = ControlNet.from_pretrained(\n",
    "    'lllyasviel/sd-controlnet-openpose',\n",
    "    torch_dtype=torch.float16\n",
    ").to('cuda')\n",
    "\n",
    "masker = Masker()\n",
    "openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
    "\n",
    "image = load_image('../images/models/APL.EC.T.2_18.jpg')\n",
    "mask_image = masker.get_binary_mask(image, return_pil=True)\n",
    "example_image = load_image('../images/garments/Ao_thun_cotton_in_Care_Share_moi-9.jpg')\n",
    "control_image = openpose(image)\n",
    "\n",
    "image.thumbnail((512, 512), PIL.Image.Resampling.LANCZOS)\n",
    "mask_image.thumbnail((512, 512), PIL.Image.Resampling.LANCZOS)\n",
    "example_image.thumbnail((512, 512), PIL.Image.Resampling.LANCZOS)\n",
    "control_image.resize(image.size)\n",
    "\n",
    "image = pipe(\n",
    "    image=init_image,\n",
    "    mask_image=mask_image,\n",
    "    example_image=example_image,\n",
    "    control_image=control_image\n",
    ").images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev venv",
   "language": "python",
   "name": "dev-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
