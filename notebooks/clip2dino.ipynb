{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fb63d7-ff6e-4f28-ad3b-a0e45129f445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9916ae8-c2b5-4572-8888-dc94a4a66aef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is available (SwiGLU)\")\n",
      "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)\n",
      "  warnings.warn(\"xFormers is available (Attention)\")\n",
      "/home/ubuntu/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)\n",
      "  warnings.warn(\"xFormers is available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    * Input image\n",
    "    * Get DINOv2 embedding given the image (main part)\n",
    "    * Plug it into IP-Adapter parameters --> Boom!\n",
    "\"\"\"\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load model\n",
    "dinov2 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
    "dinov2.to(device)\n",
    "\n",
    "img = Image.open('../images/garments/Ao_thun_oversize_84RISING_-_Vit_Donald_Disneyshadow1.jpg')\n",
    "\n",
    "# image preprocessing\n",
    "transform = T.Compose([\n",
    "    T.Resize(224),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "img = transform(img)[:3].unsqueeze(0)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    features = dinov2(img.to(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9360676-d8a6-40e7-807b-ad8aa421545c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/Markus-Pobitzer/Inpainting-Tutorial/tree/main\n",
    "class Masker:\n",
    "    def __init__(self, model_id='mattmdjaga/segformer_b2_clothes'): # SegFormer\n",
    "        self.processor = SegformerImageProcessor.from_pretrained(model_id)\n",
    "        self.model = AutoModelForSemanticSegmentation.from_pretrained(model_id)\n",
    "\n",
    "    def get_binary_mask(self, image, return_pil=False):\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\")        \n",
    "        outputs = self.model(**inputs)\n",
    "        logits = outputs.logits.cpu()\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            logits,\n",
    "            size=image.size[::-1],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )\n",
    "        pred_seg = upsampled_logits.argmax(dim=1)[0] # Dunno how this line work\n",
    "        np_image = np.array(image)\n",
    "        np_image[pred_seg != 4] = 0\n",
    "        np_image[pred_seg == 4] = 255\n",
    "        binary_mask = ((pred_seg == 4) * 255).numpy().astype(np.uint8)\n",
    "\n",
    "        return Image.fromarray(binary_mask) if return_pil else binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb07313-442d-4de8-9281-6bbfb95a82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_on(original_image: PIL.Image.Image, \n",
    "           mask_image: PIL.Image.Image,\n",
    "           ip_image: PIL.Image.Image,\n",
    "           prompt: Text,\n",
    "           control_image: Union[PIL.Image.Image] = None,\n",
    "           **kwargs) -> List[PIL.Image.Image]:\n",
    "    images = pipeline(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=kwargs.get('negative_prompt'),\n",
    "        image=original_image,\n",
    "        mask_image=mask_image,\n",
    "        control_image=control_image,\n",
    "        guidance_scale=kwargs.get('guidance_scale'),\n",
    "        width=kwargs.get('width'), # output width\n",
    "        height=kwargs.get('height'), # output height\n",
    "        ip_adapter_image=ip_image,\n",
    "        generator=kwargs.get('seed'),\n",
    "        num_images_per_prompt=kwargs.get('num_images'),\n",
    "        strength=kwargs.get('strength'),\n",
    "        num_inference_steps=kwargs.get('num_inference_steps'),\n",
    "        padding_mask_crop=kwargs.get('padding_mask_crop')\n",
    "    ).images\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70871d48-6300-44f4-8c01-233f2929c2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    'lllyasviel/sd-controlnet-openpose',\n",
    "    torch_dtype=torch.float16\n",
    ").to('cuda')\n",
    "\n",
    "inpainting_base_model = 'runwayml/stable-diffusion-inpainting'\n",
    "pipeline = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n",
    "    inpainting_base_model, \n",
    "    variant='fp16',\n",
    "    torch_dtype=torch.float16,\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None\n",
    ").to('cuda')\n",
    "\n",
    "ip_adapter_model = 'h94/IP-Adapter'\n",
    "pipeline.load_ip_adapter(ip_adapter_model, subfolder='models', weight_name='ip-adapter_sd15.bin')\n",
    "pipeline.set_ip_adapter_scale(1.0)\n",
    "\n",
    "masker = Masker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8f094-4723-494d-a641-4eabf485ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image('../images/male_model.jpg')\n",
    "ip_image = load_image('../tmp/img4.jpg')\n",
    "mask_image = masker.get_binary_mask(image, return_pil=True)\n",
    "\n",
    "assert isinstance(image, PIL.Image.Image)\n",
    "assert isinstance(ip_image, PIL.Image.Image)\n",
    "assert isinstance(mask_image, PIL.Image.Image)\n",
    "\n",
    "INPUT_IMAGE_SIZE = (512, 768)\n",
    "SQUARE_IMAGE_SIZE = (224, 224)\n",
    "image = image.resize(INPUT_IMAGE_SIZE)\n",
    "mask_image = mask_image.resize(INPUT_IMAGE_SIZE)\n",
    "\n",
    "make_image_grid([ip_image.resize(INPUT_IMAGE_SIZE), image, mask_image], rows=1, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf27f443-318a-4ccc-ad8a-8b3be07cb45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = '3d, cartoon, anime, sketches, (worst quality, bad quality, cropped:1.4) ((monochrome) ), \\\n",
    "((grayscale) ), (bad-hands-5:1) , (badhandv4) , (easynegative:0.8) , (bad-artist-anime:) , (bad-artist:) , (bad_prompt:) , \\\n",
    "(bad-picture-chill-75v:) , (bad_prompt_version2:) , (bad_quality:) , Asian-Less-Neg bad-hands-5 bad_pictures \\\n",
    "bad artist -neg CyberRealistic_Negative-neg negative_hand-neg ng_deepnegative_v1_75t, verybadimagenegative_v1. 3, lowres, \\\n",
    "low quality, jpeg, artifacts, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, \\\n",
    "poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, \\\n",
    "gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long neck, \\\n",
    "drawing, painting, crayon, sketch, graphite, impressionist, noisy, soft'\n",
    "\n",
    "pipe_config = {\n",
    "    'negative_prompt': negative_prompt,\n",
    "    'guidance_scale': 9.0,\n",
    "    'num_images': 3,\n",
    "    'strength': 1,\n",
    "    'num_inference_steps': 50,\n",
    "    'seed': torch.Generator(device=\"cuda\").manual_seed(1337),\n",
    "    'width': INPUT_IMAGE_SIZE[0],\n",
    "    'height': INPUT_IMAGE_SIZE[1],\n",
    "    'padding_mask_crop': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8b2d5-a0b5-498e-8fbd-98ff687e86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n",
    "pose = openpose(image)\n",
    "make_image_grid([image, pose], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af5aaa-ea10-4b95-b612-40745948d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = try_on(image, mask_image, ip_image, prompt=\"a sks black polo neck t-shirt\", control_image=pose, **pipe_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev venv",
   "language": "python",
   "name": "dev-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
